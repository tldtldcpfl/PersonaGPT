{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch, os, pickle\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np, random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# utils.py\n",
    "import torch, os, pickle, matplotlib.pyplot as plt\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def process_conv(row, tokenizer, eos = True, make_flat=True):\n",
    "    if eos:\n",
    "        conv = list([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row])\n",
    "    else: conv = list([tokenizer.encode(x) for x in row])\n",
    "    if make_flat: conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "def split_by_index(seq, sep):\n",
    "    result = []\n",
    "    for el in seq:\n",
    "        result.append(el)\n",
    "        if el == sep:\n",
    "            yield result\n",
    "            result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "# https://huggingface.co/microsoft/DialoGPT-medium?text=Hey+my+name+is+Julien%21+How+are+you%3F\n",
    "\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          pad_token='<|endoftext|>', cls_token='<|cls|>',\n",
    "                                                sep_token='<|sep|>'\n",
    "                                         )\n",
    "\n",
    "p1_tok, p2_tok, start_tok = tokenizer.encode('<|p1|>')[0], tokenizer.encode('<|p2|>')[0], tokenizer.encode('<|start|>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Oct 12 11:40:29 2020\n",
    "\n",
    "@author: af1tang\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "import torch, os, pickle\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np, random\n",
    "import pandas as pd\n",
    "\n",
    "#from load_configs import model, tokenizer, opts, device, data_path, save_path\n",
    "#from utils import *\n",
    "\n",
    "## Persona Preprocess ##\n",
    "def preprocess_convai(filename):\n",
    "    raw_data = open(filename).read().strip().split('\\n')\n",
    "    data, count = {}, 0\n",
    "    curr_convo, curr_ps, curr_pt = [], [], []\n",
    "    indices = []\n",
    "    \n",
    "    person_a = 'your persona'\n",
    "    person_b = \"partner's persona\"\n",
    "    with tqdm(total = len(raw_data)) as pbar:\n",
    "        turn_count, ctx_count = 1,0 #init cycle\n",
    "        for idx, line in enumerate(raw_data):\n",
    "            if person_a in line[0:20]:\n",
    "                if (turn_count != 0) and (len(curr_ps)>1 and len(curr_pt)>1 and len(curr_convo)>1):\n",
    "                    if idx > 1:\n",
    "                        if curr_convo[0] == '__SILENCE__' :\n",
    "                            p1 = curr_ps; p2 = curr_pt; curr_convo = curr_convo[1:]\n",
    "                        else:\n",
    "                            p1 = curr_pt; p2 = curr_ps\n",
    "                        data[count] = { 'inp': process_conv([curr_convo[0]], tokenizer),\n",
    "                                        'labels': process_conv(curr_convo[1:],tokenizer), #to_data(torch.cat(curr_convo,dim=-1)[0]), \n",
    "                                       'p_src': process_conv(p1, tokenizer,make_flat=False), #to_data(torch.cat(curr_ps,dim=-1)[0]),\n",
    "                                       'p_trg': process_conv(p2, tokenizer, make_flat=False)}#to_data(torch.cat(curr_pt,dim=-1)[0])}\n",
    "                        count+=1\n",
    "                    curr_convo, curr_ps, curr_pt = [], [], []\n",
    "                    turn_count=0\n",
    "\n",
    "                words = line.split()\n",
    "                turn_id, words = int(words[0]), ' '.join(words[3:])\n",
    "                curr_ps.append(words)\n",
    "\n",
    "                ctx_count +=1\n",
    "                assert ctx_count == turn_id\n",
    "                \n",
    "            elif person_b in line[0:20]:\n",
    "                if (turn_count != 0) and (len(curr_ps)>1 and len(curr_pt)>1 and len(curr_convo)>1):\n",
    "                    if idx > 1:\n",
    "                        if curr_convo[0] == '__SILENCE__' :\n",
    "                            p1 = curr_ps; p2 = curr_pt; curr_convo = curr_convo[1:]\n",
    "                        else:\n",
    "                            p1 = curr_pt; p2 = curr_ps\n",
    "                        data[count] = { 'inp': process_conv([curr_convo[0]], tokenizer),\n",
    "                                        'labels': process_conv(curr_convo[1:],tokenizer), #to_data(torch.cat(curr_convo,dim=-1)[0]), \n",
    "                                       'p_src': process_conv(p1, tokenizer,make_flat=False), #to_data(torch.cat(curr_ps,dim=-1)[0]),\n",
    "                                       'p_trg': process_conv(p2, tokenizer, make_flat=False)}#to_data(torch.cat(curr_pt,dim=-1)[0])}\n",
    "                        count+=1\n",
    "                    curr_convo, curr_ps, curr_pt = [], [], []\n",
    "                    turn_count=0\n",
    "                words = line.split()\n",
    "                turn_id, words = int(words[0]), ' '.join(words[3:])\n",
    "                curr_pt.append(words)\n",
    "\n",
    "                ctx_count +=1\n",
    "                assert ctx_count == turn_id\n",
    "\n",
    "                \n",
    "            else:\n",
    "                if ctx_count !=0:\n",
    "                    turn_count = ctx_count *1 \n",
    "                    ctx_count =0\n",
    "                    indices.append(idx)\n",
    "                        \n",
    "                src_line, trg_line = line.split('\\t')\n",
    "                src_words = src_line.split()\n",
    "                src_idx, src_line = src_words[0], ' '.join(src_words[1:])\n",
    "\n",
    "                curr_convo.append(src_line) \n",
    "                curr_convo.append(trg_line)#turn)\n",
    "                \n",
    "                turn_count +=1\n",
    "                assert turn_count == int(src_idx)\n",
    "                \n",
    "            pbar.update(1)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def convert_to_XY(old_data):\n",
    "    data = []\n",
    "    print(\"building training set...\")\n",
    "    for i in range(len(old_data)):\n",
    "        p1 = [tokenizer.decode(p) for p in old_data[i]['p_src']]\n",
    "        p2 = [tokenizer.decode(p) for p in old_data[i]['p_trg']]\n",
    "\n",
    "        convo = old_data[i]['inp'] + old_data[i]['labels']\n",
    "        dialog_hx = list(split_by_index(convo,tokenizer.eos_token_id))\n",
    "        #if len(dialog_hx) < 30:\n",
    "        dialog_hx = [line + [tokenizer.eos_token_id] for line in dialog_hx[:20]] # limit by max len of convo\n",
    "        p1_ctx = tokenizer.encode(''.join(['<|p1|>'] + p1 + ['<|sep|>'] + ['<|start|>']))\n",
    "        p2_ctx = tokenizer.encode(''.join(['<|p2|>'] + p2 + ['<|sep|>'] + ['<|start|>']))\n",
    "        for t in range(len(dialog_hx)):\n",
    "            x = dialog_hx[:t]\n",
    "            y = dialog_hx[t]\n",
    "            if t == 0:\n",
    "                x = p1_ctx[:-1] \n",
    "                y = [p1_ctx[-1]] + y\n",
    "            elif t %2 ==0:\n",
    "                x = p1_ctx + flatten(x)\n",
    "            else:\n",
    "                x = p2_ctx + flatten(x)\n",
    "            data.append((x,y))\n",
    "    return data\n",
    "\n",
    "def build_active_data():\n",
    "    df = pd.read_csv('active_learning_data.csv')\n",
    "    X, y = df['context'].tolist(), df['response'].tolist()\n",
    "    X, y = [tokenizer.encode(x) for x in X], [tokenizer.encode(yy) for yy in y]\n",
    "    data = {'X':X, 'y':y}\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 292175/292175 [01:11<00:00, 4090.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_path=  'train_both_original_no_cands.txt'\n",
    "train_data = preprocess_convai(train_data_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building training set...\n"
     ]
    }
   ],
   "source": [
    "train_data = convert_to_XY(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 16779/16779 [00:03<00:00, 4741.95it/s]\n"
     ]
    }
   ],
   "source": [
    "val_data_path = 'valid_both_original_no_cands.txt'\n",
    "val_data = preprocess_convai(val_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building training set...\n"
     ]
    }
   ],
   "source": [
    "val_data = convert_to_XY(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir('./trimmed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data in trimmed_data folder\n",
    "with open('./trimmed_data/train_data.pkl', mode='wb') as f:\n",
    "    pickle.dump(train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trimmed_data/val_data.pkl', mode='wb') as f:\n",
    "    pickle.dump(val_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_data = build_active_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trimmed_data/active_data.pkl', mode='wb') as f:\n",
    "    pickle.dump(active_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
